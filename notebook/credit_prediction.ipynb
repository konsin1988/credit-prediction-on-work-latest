{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a916e5-b7ad-4e92-a118-a189b278e91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\d.konshin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import optuna\n",
    "\n",
    "from giskard import Dataset, Model, scan, testing\n",
    "import pickle\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import warnings\n",
    "\n",
    "# import clickhouse_connect\n",
    "import psycopg2\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import clickhouse_connect\n",
    "\n",
    "import mlflow \n",
    "import mlflow.catboost\n",
    "import mlflow.sklearn\n",
    "import mlflow.data\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26599f47-ec65-4763-a45c-6c8cf27ef447",
   "metadata": {},
   "source": [
    "# Env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af346c89-40de-4503-90ee-8563280f7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SPARK_COMPAT_VERSION = os.getenv('SPARK_COMPAT_VERSION')\n",
    "SCALA_COMPAT_VERSION = os.getenv('SCALA_COMPAT_VERSION')\n",
    "CATBOOST_SPARK_VERSION = os.getenv('CATBOOST_SPARK_VERSION')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "POSTGRESQL_PORT = os.getenv('POSTGRESQL_PORT')\n",
    "CLICKHOUSE_PORT = os.getenv('CLICKHOUSE_PORT')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "\n",
    "ACCESS_KEY=os.getenv('MINIO_ROOT_USER')\n",
    "SECRET_KEY=os.getenv('MINIO_ROOT_PASSWORD')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d0ce3-959f-4d48-83ce-f3354b9f52f5",
   "metadata": {},
   "source": [
    "## Save vars as json file (for airflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a920561-7318-4224-9a85-a3e9770d9991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "env_vars = {\n",
    "    'CP_SPARK_COMPAT_VERSION': SPARK_COMPAT_VERSION,\n",
    "    'CP_SCALA_COMPAT_VERSION': SCALA_COMPAT_VERSION,\n",
    "    'CP_CATBOOST_SPARK_VERSION': CATBOOST_SPARK_VERSION,\n",
    "    'CP_DB_HOST': DB_HOST,\n",
    "    'CP_POSTGRESQL_PORT': POSTGRESQL_PORT,\n",
    "    'CP_CLICKHOUSE_PORT': CLICKHOUSE_PORT,\n",
    "    'CP_DB_USER': DB_USER,\n",
    "    'CP_DB_PASSWORD': DB_PASSWORD,\n",
    "    'CP_DB_NAME': DB_NAME,\n",
    "    'CP_ACCESS_KEY': ACCESS_KEY,\n",
    "    'CP_SECRET_KEY': SECRET_KEY\n",
    "}\n",
    "\n",
    "with open('../airflow/env_vars.json', 'w') as file:\n",
    "    json.dump(env_vars, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590242f9-cf3b-4fb8-ba6d-2a9acf890467",
   "metadata": {},
   "source": [
    "# Set feature and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4611343e-78af-4c09-875b-a4d3ba88e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_TYPES = {\n",
    "    'age': 'category',\n",
    "    'sex': 'category',\n",
    "    'job': 'category',\n",
    "    'housing': 'category',\n",
    "    'credit_amount': 'numeric',\n",
    "    'duration': 'numeric'\n",
    "}\n",
    "\n",
    "TARGET_COLUMN_NAME = 'default'\n",
    "FEATURE_COLUMNS = [i for i in COLUMN_TYPES.keys()]\n",
    "FEATURE_TYPES = {i: COLUMN_TYPES[i] for i in COLUMN_TYPES if i != TARGET_COLUMN_NAME}\n",
    "\n",
    "COLUMNS_TO_SCALE = [key for key in COLUMN_TYPES.keys() if COLUMN_TYPES[key] == \"numeric\"]\n",
    "COLUMNS_TO_ENCODE = [key for key in COLUMN_TYPES.keys() if COLUMN_TYPES[key] == \"category\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61e3a8-7dcc-4786-a4de-7b0617a3f600",
   "metadata": {},
   "source": [
    "# Connect to db (data) and get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99036d0f-8e52-48a9-9eab-e48bb79b52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = clickhouse_connect.get_client(host = CLICKHOUSE_HOST, \n",
    "#                                        port = CLICKHOUSE_PORT, \n",
    "#                                        user = CLICKHOUSE_USER, \n",
    "#                                        password = CLICKHOUSE_PASSWORD)\n",
    "# query = fr'''\n",
    "# select {reduce(lambda a,b: a + ', ' + b, FEATURE_COLUMNS)}\n",
    "# from credit.credit\n",
    "# '''\n",
    "# X = pd.DataFrame(client.query(query).named_results())\n",
    "# X\n",
    "\n",
    "job_list = {\n",
    "    0: 'unskilled and non-resident', \n",
    "    1: 'unskilled and resident', \n",
    "    2: 'skilled', \n",
    "    3: 'highly skilled'\n",
    "}\n",
    "\n",
    "\n",
    "## Postgresql ==============================================\n",
    "conn = psycopg2.connect(dbname='credit',\n",
    "                                user=DB_USER,\n",
    "                                password=DB_PASSWORD,\n",
    "                                host='localhost',\n",
    "                                port=POSTGRESQL_PORT)\n",
    "cur = conn.cursor()\n",
    "cur.execute(f\"SELECT {reduce(lambda a,b: a + ', ' + b, FEATURE_COLUMNS)} FROM credit;\")\n",
    "X = (\n",
    "    pd\n",
    "    .DataFrame(cur.fetchall(), columns=FEATURE_COLUMNS)\n",
    "    .assign(job = lambda x: x['job'].apply(lambda x: job_list[x]))\n",
    ")\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(dbname='credit',\n",
    "                                user=DB_USER,\n",
    "                                password=DB_PASSWORD,\n",
    "                                host='localhost',\n",
    "                                port=POSTGRESQL_PORT)\n",
    "cur = conn.cursor()\n",
    "cur.execute(f'SELECT cr.\"{TARGET_COLUMN_NAME}\" FROM credit cr;')\n",
    "y = [x[0] for x in cur.fetchall()]\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "df = X.join(pd.DataFrame(y, columns = [TARGET_COLUMN_NAME]))\n",
    "#=================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38048b-ed2b-47d1-86f5-0cdbc3043c10",
   "metadata": {},
   "source": [
    "# MLflow connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d8450d7-e83c-45b3-9259-3c4e16df79bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI http://localhost:5000/\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "mlflow.set_tracking_uri('http://localhost:5000/')\n",
    "print(\"URI\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0894caf9-1795-4e10-a5d4-d17d1da8dbc2",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed2f34df-25df-4117-856f-2a9753e202fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps = [\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "# \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, COLUMNS_TO_SCALE),\n",
    "        (\"cat\", categorical_transformer, COLUMNS_TO_ENCODE)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3279f01f-7a24-46f0-ad37-bf59f305329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_preproccessed = preprocessor.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size = 0.25,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ef5dc8-ce11-4059-b53e-324179d183d3",
   "metadata": {},
   "source": [
    "### Optimize hyperparams (optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cea44e9-bd2a-403b-a53b-3391b3dcab6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 17:10:32,070] A new study created in memory with name: params_opt_20250513-171032\n",
      "[I 2025-05-13 17:10:43,725] Trial 0 finished with value: 0.6739943536350722 and parameters: {'objective': 'CrossEntropy', 'colsample_bylevel': 0.050460636371246856, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS'}. Best is trial 0 with value: 0.6739943536350722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run run_13.05.2025_17:10:32 at: http://localhost:5000/#/experiments/94/runs/af56f65f33724edeb5c6381abbcdf213\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 17:10:46,650] Trial 1 finished with value: 0.7019864175552798 and parameters: {'objective': 'Logloss', 'colsample_bylevel': 0.03302150385278185, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian'}. Best is trial 1 with value: 0.7019864175552798.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run run_13.05.2025_17:10:43 at: http://localhost:5000/#/experiments/94/runs/7f21d7e7b585414e9821a7c52386a49a\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 17:11:12,356] Trial 2 finished with value: 0.701995408582235 and parameters: {'objective': 'CrossEntropy', 'colsample_bylevel': 0.07860651787615978, 'depth': 9, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS'}. Best is trial 2 with value: 0.701995408582235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run run_13.05.2025_17:10:46 at: http://localhost:5000/#/experiments/94/runs/65e20b0c626b4819a7a2bd3a4747a338\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 17:11:30,349] Trial 3 finished with value: 0.7129734524944107 and parameters: {'objective': 'CrossEntropy', 'colsample_bylevel': 0.06665377054949738, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian'}. Best is trial 3 with value: 0.7129734524944107.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run run_13.05.2025_17:11:12 at: http://localhost:5000/#/experiments/94/runs/d7027ac4740340aba6a9de956cc59827\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 499.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run params_opt_13.05.2025_17:10 at: http://localhost:5000/#/experiments/94/runs/c20ca4c47f324ce899508f08d4b84267\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/94\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):    \n",
    "    # CatBoostClassifier hyperparams\n",
    "    params = {\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\n",
    "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "        ),\n",
    "        \"used_ram_limit\": \"3gb\",\n",
    "    }\n",
    "    with mlflow.start_run(run_name = f'run_{datetime.now().strftime('%d.%m.%Y_%H:%M:%S')}', nested=True):\n",
    "        \n",
    "        # model\n",
    "        mlflow.log_params(params)\n",
    "        estimator = CatBoostClassifier(**params, verbose=False)\n",
    "        \n",
    "        accuracy = cross_val_score(estimator, X_preproccessed, y, cv=3, scoring= 'accuracy').mean()\n",
    "        mlflow.log_metric('Accuracy', accuracy) \n",
    "        return accuracy\n",
    "\n",
    "\n",
    "experiment_name = f'credit_pred_{datetime.now().strftime('01.%m.%Y')}'\n",
    "try:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "except:\n",
    "    pass\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name = f'params_opt_{datetime.now().strftime('%d.%m.%Y_%H:%M')}') as run:\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=f\"params_opt_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    # Hyperparams searching\n",
    "    study.optimize(objective, n_trials=4)\n",
    "    \n",
    "    # best result is\n",
    "    params = study.best_params\n",
    "\n",
    "    estimator = CatBoostClassifier(**params, verbose=False)  \n",
    "    catboostclassifier = Pipeline(steps = [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", estimator)\n",
    "    ])\n",
    "    # catboostclassifier = CatBoostClassifier(**params, verbose=False)\n",
    "    catboostclassifier.fit(X_train, y_train)\n",
    "    \n",
    "    pred_test = catboostclassifier.predict(X_test)\n",
    "    signature = infer_signature(X_test, pred_test)\n",
    "\n",
    "    \n",
    "    metrics = {'accuracy': accuracy_score(pred_test, y_test),\n",
    "                'precision': precision_score(pred_test, y_test),\n",
    "                'recall': recall_score(pred_test, y_test),\n",
    "                'f1': f1_score(pred_test, y_test),\n",
    "                'roc_auc': roc_auc_score(y_test, catboostclassifier.predict_proba(X_test)[:,1])\n",
    "              }\n",
    "\n",
    "    \n",
    "    input_example = X_train.iloc[[0], :]\n",
    "    mlflow.models.infer_signature(input_example, 0, params)\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    dataset = mlflow.data.from_pandas(df, name='german_credit', targets='default')\n",
    "    mlflow.log_input(dataset)\n",
    "\n",
    "    mlflow.sklearn.log_model(sk_model=catboostclassifier, \n",
    "                              artifact_path='catboostclassifier', \n",
    "                              signature=signature,\n",
    "                              input_example=input_example\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba97f7-8bdd-48b3-b45a-31fc1e1ccea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='run ' + datetime.now().strftime('%d.%m.%Y %H:%M')) as run:\n",
    "    estimator = CatBoostClassifier(**params, verbose=False)\n",
    "    \n",
    "    catboostclassifier = Pipeline(steps = [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", estimator)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # catboostclassifier = CatBoostClassifier(**params, verbose=False)\n",
    "    catboostclassifier.fit(X_train, y_train)\n",
    "    \n",
    "    pred_test = catboostclassifier.predict(X_test)\n",
    "    signature = infer_signature(X_test, pred_test)\n",
    "\n",
    "    \n",
    "    metrics = {'accuracy': accuracy_score(pred_test, y_test),\n",
    "                'precision': precision_score(pred_test, y_test),\n",
    "                'recall': recall_score(pred_test, y_test),\n",
    "                'f1': f1_score(pred_test, y_test),\n",
    "                'roc_auc': roc_auc_score(y_test, catboostclassifier.predict_proba(X_test)[:,1])\n",
    "              }\n",
    "\n",
    "    \n",
    "    input_example = X_train.iloc[[0], :]\n",
    "    mlflow.models.infer_signature(input_example, 0, params)\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    dataset = mlflow.data.from_pandas(df, name='german_credit', targets='default')\n",
    "    mlflow.log_input(dataset)\n",
    "    # mlflow.evaluate(data=df, model_type=\"classifier\", targets=TARGET_COLUMN_NAME, model=catboostclassifier)\n",
    "    \n",
    "    mlflow.sklearn.log_model(catboostclassifier, \n",
    "                              'catboostclassifier', \n",
    "                              signature=signature,\n",
    "                              input_example=input_example\n",
    "                             )\n",
    "    # mlflow.sklearn.save_model(sk_model=catboostclassifier, \n",
    "    #                            path='catboostclassifier',\n",
    "    #                           signature=signature,\n",
    "    #                           input_example=input_example\n",
    "    #                          )\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe02a42-0aeb-4bdf-993c-eb9ffb6d7539",
   "metadata": {},
   "source": [
    "## Wrap dataset with Giskard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1565001-06d6-460d-b5e2-7e114bf2e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.concat([X_test, y_test], axis = 1)\n",
    "giskard_dataset = Dataset(\n",
    "    df = raw_data,\n",
    "    target=TARGET_COLUMN_NAME,\n",
    "    name = \"German credit scoring dataset\",\n",
    "    cat_columns=COLUMNS_TO_ENCODE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7daf2c4-33de-4df9-bbdb-111d1725bc65",
   "metadata": {},
   "source": [
    "## Wrap model with Giskard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1fb196-bd6e-4385-bc32-49165cb2cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "giskard_model = Model(\n",
    "    model=catboostclassifier,\n",
    "    model_type=\"classification\",     # Either regression, classification or text_generation.\n",
    "    name=\"Chunk classification\",\n",
    "    classification_labels=catboostclassifier.classes_,  # Their order MUST be identical to the prediction_function's output order\n",
    "    feature_names=FEATURE_COLUMNS     # Default: all columns of your dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36044d8-cbe6-437e-9a9d-9d28e1f6aa3c",
   "metadata": {},
   "source": [
    "## Scan model with Giskard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c231a9-5af7-4a1a-9890-efe3e96e7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scan(giskard_model, giskard_dataset, verbose=False)\n",
    "results.to_html(\"giskard_scan_result.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70fd3a0-c172-4a88-919a-aa3b2656c1b1",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed2525a2-5b60-4346-9bc2-59e8564de9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(catboostclassifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7b143-5840-46e3-acb4-96baea5b1f40",
   "metadata": {},
   "source": [
    "# Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c52214d5-82e8-4c73-812a-2c373f72c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket credit-model-new-new\n",
      "model.pkl successfully uploaded as object model.pkl to bucket credit-model-new-new\n"
     ]
    }
   ],
   "source": [
    "def s3_upload_model():\n",
    "    # Create a client with the MinIO server playground, its access key\n",
    "    # and secret key.\n",
    "    client = Minio(\"localhost:9099\",\n",
    "        access_key=ACCESS_KEY,\n",
    "        secret_key=SECRET_KEY,\n",
    "        secure=False\n",
    "    )\n",
    "\n",
    "    # The file to upload, change this path if needed\n",
    "    source_file = \"model.pkl\"\n",
    "\n",
    "    # The destination bucket and filename on the MinIO server\n",
    "    bucket_name = \"credit-model-new-new\"\n",
    "    destination_file = \"model.pkl\"\n",
    "\n",
    "    # Make the bucket if it doesn't exist.\n",
    "    found = client.bucket_exists(bucket_name)\n",
    "    if not found:\n",
    "        client.make_bucket(bucket_name)\n",
    "        print(\"Created bucket\", bucket_name)\n",
    "    else:\n",
    "        print(\"Bucket\", bucket_name, \"already exists\")\n",
    "\n",
    "    # Upload the file, renaming it in the process\n",
    "    client.fput_object(\n",
    "        bucket_name, destination_file, source_file,\n",
    "    )\n",
    "    print(\n",
    "        source_file, \"successfully uploaded as object\",\n",
    "        destination_file, \"to bucket\", bucket_name,\n",
    "    )\n",
    "\n",
    "try:\n",
    "    s3_upload_model()\n",
    "except S3Error as exc:\n",
    "    print(\"error occurred.\", exc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
